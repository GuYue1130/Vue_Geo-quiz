{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f832acf3",
      "metadata": {
        "id": "f832acf3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# function about ploting figures\n",
        "def plot_hist_marginals(data, lims=None, gt=None):\n",
        "    \n",
        "    n_bins = int(np.sqrt(data.shape[0]))\n",
        "\n",
        "    if data.ndim == 1:\n",
        "\n",
        "        fig, ax = plt.subplots(1, 1)\n",
        "        ax.hist(data, n_bins, normed=True)\n",
        "        ax.set_ylim([0, ax.get_ylim()[1]])\n",
        "        if lims is not None: ax.set_xlim(lims)\n",
        "        if gt is not None: ax.vlines(gt, 0, ax.get_ylim()[1], color='r')\n",
        "\n",
        "    else:\n",
        "\n",
        "        n_dim = data.shape[1]\n",
        "        fig, ax = plt.subplots(n_dim, n_dim)\n",
        "        ax = np.array([[ax]]) if n_dim == 1 else ax\n",
        "\n",
        "        if lims is not None:\n",
        "            lims = np.asarray(lims)\n",
        "            lims = np.tile(lims, [n_dim, 1]) if lims.ndim == 1 else lims\n",
        "\n",
        "        for i in range(n_dim):\n",
        "            for j in range(n_dim):\n",
        "\n",
        "                if i == j:\n",
        "                    ax[i, j].hist(data[:, i], n_bins, normed=True)\n",
        "                    ax[i, j].set_ylim([0, ax[i, j].get_ylim()[1]])\n",
        "                    if lims is not None: ax[i, j].set_xlim(lims[i])\n",
        "                    if gt is not None: ax[i, j].vlines(gt[i], 0, ax[i, j].get_ylim()[1], color='r')\n",
        "\n",
        "                else:\n",
        "                    ax[i, j].plot(data[:, i], data[:, j], 'k.', ms=2)\n",
        "                    if lims is not None:\n",
        "                        ax[i, j].set_xlim(lims[i])\n",
        "                        ax[i, j].set_ylim(lims[j])\n",
        "                    if gt is not None: ax[i, j].plot(gt[i], gt[j], 'r.', ms=8)\n",
        "\n",
        "    plt.show(block=False)\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def one_hot_encode(labels, n_labels):\n",
        "   \n",
        "    assert np.min(labels) >= 0 and np.max(labels) < n_labels\n",
        "\n",
        "    y = np.zeros([labels.size, n_labels])\n",
        "    y[range(labels.size), labels] = 1\n",
        "\n",
        "    return y\n",
        "\n",
        "def logit(x):\n",
        "   \n",
        "    return np.log(x / (1.0 - x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef3a310d",
      "metadata": {
        "id": "ef3a310d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "class MNIST:\n",
        "\n",
        "\n",
        "    alpha = 1.0e-6\n",
        "\n",
        "    # construct dataset\n",
        "    class Data:\n",
        "\n",
        "        def __init__(self, data, logit, dequantize, rng):\n",
        "\n",
        "            x = self._dequantize(data[0], rng) if dequantize else data[0]  \n",
        "            self.x = self._logit_transform(x) if logit else x              \n",
        "            self.labels = data[1]                                         \n",
        "            self.y = util.one_hot_encode(self.labels, 10)                  \n",
        "            self.N = self.x.shape[0]                                       \n",
        "\n",
        "        @staticmethod\n",
        "        # Add noise to pixels to dequantize them dequantization\n",
        "        def _dequantize(x, rng):\n",
        "\n",
        "            return x + rng.rand(*x.shape) / 256.0\n",
        "\n",
        "        @staticmethod\n",
        "        # Transform pixel values ​​to unconstrained using logit\n",
        "        def _logit_transform(x):\n",
        "\n",
        "            return util.logit(MNIST.alpha + (1 - 2*MNIST.alpha) * x)\n",
        "\n",
        "    def __init__(self, logit=True, dequantize=True):\n",
        "\n",
        "        # load dataset\n",
        "        f = gzip.open(datasets.root + '/mnist.pkl.gz', 'rb')\n",
        "        trn,val, tst = pickle.load(f, encoding='latin1')\n",
        "        f.close()\n",
        "\n",
        "        rng = np.random.RandomState(42)\n",
        "        self.trn = self.Data(trn, logit, dequantize, rng)\n",
        "        self.val = self.Data(val, logit, dequantize, rng)\n",
        "        self.tst = self.Data(tst, logit, dequantize, rng)\n",
        "\n",
        "        im_dim = int(np.sqrt(self.trn.x.shape[1]))\n",
        "        self.n_dims = (1, im_dim, im_dim)\n",
        "        self.n_labels = self.trn.y.shape[1]\n",
        "        self.image_size = [im_dim, im_dim]\n",
        "\n",
        "    # Plot a histogram of pixel values ​​or a specific pixel\n",
        "    def show_pixel_histograms(self, split, pixel=None):\n",
        "\n",
        "        data_split = getattr(self, split, None)\n",
        "        if data_split is None:\n",
        "            raise ValueError('Invalid data split')\n",
        "\n",
        "        if pixel is None:\n",
        "            data = data_split.x.flatten()\n",
        "\n",
        "        else:\n",
        "            row, col = pixel\n",
        "            idx = row * self.image_size[0] + col\n",
        "            data = data_split.x[:, idx]\n",
        "\n",
        "        n_bins = int(np.sqrt(data_split.N))\n",
        "        fig, ax = plt.subplots(1, 1)\n",
        "        ax.hist(data, n_bins, normed=True)\n",
        "        plt.show()\n",
        "\n",
        "    def show_images(self, split):\n",
        "        \n",
        "        # get split\n",
        "        data_split = getattr(self, split, None)\n",
        "        if data_split is None:\n",
        "            raise ValueError('Invalid data split')\n",
        "\n",
        "        # display images\n",
        "        util.disp_imdata(data_split.x, self.image_size, [6, 10])\n",
        "\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f60b31",
      "metadata": {
        "id": "28f60b31"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def logit(x, eps=1e-5):\n",
        "    x.clamp_(eps, 1 - eps)\n",
        "    return x.log() - (1 - x).log()\n",
        "# Convert one-hot encoding\n",
        "def one_hot(x, label_size):\n",
        "    out = torch.zeros(len(x), label_size).to(x.device)\n",
        "    out[torch.arange(len(x)), x] = 1\n",
        "    return out\n",
        "# load dataset\n",
        "def load_dataset(name):\n",
        "    exec('from datasets.{} import {}'.format(name.lower(), name))\n",
        "    return locals()[name]\n",
        "\n",
        "\n",
        "# load MNIST dataset\n",
        "def fetch_dataloaders(dataset_name, batch_size, device, flip_toy_var_order=False, toy_train_size=25000, toy_test_size=5000):\n",
        "    if dataset_name in ['MNIST']:\n",
        "        dataset = load_dataset(dataset_name)()\n",
        "\n",
        "        # join train and val data again\n",
        "        train_x = np.concatenate((dataset.trn.x, dataset.val.x), axis=0).astype(np.float32)\n",
        "        train_y = np.concatenate((dataset.trn.y, dataset.val.y), axis=0).astype(np.float32)\n",
        "\n",
        "        # construct datasets\n",
        "        train_dataset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "        test_dataset  = TensorDataset(torch.from_numpy(dataset.tst.x.astype(np.float32)),\n",
        "                                      torch.from_numpy(dataset.tst.y.astype(np.float32)))\n",
        "\n",
        "        input_dims = dataset.n_dims\n",
        "        label_size = 10\n",
        "        lam = dataset.alpha\n",
        "\n",
        "\n",
        "    train_dataset.input_dims = input_dims\n",
        "    train_dataset.input_size = int(np.prod(input_dims))\n",
        "    train_dataset.label_size = label_size\n",
        "    train_dataset.lam = lam\n",
        "\n",
        "    test_dataset.input_dims = input_dims\n",
        "    test_dataset.input_size = int(np.prod(input_dims))\n",
        "    test_dataset.label_size = label_size\n",
        "    test_dataset.lam = lam\n",
        "\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if device.type is 'cuda' else {}\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, **kwargs)\n",
        "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d644a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "e1d644a3",
        "outputId": "407702dc-128e-409b-ae99-c75551f95e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--train] [--evaluate] [--generate]\n",
            "                             [--model MODEL] [--dataset DATASET]\n",
            "                             [--batch_size BATCH_SIZE] [--n_epochs N_EPOCHS]\n",
            "                             [--start_epoch START_EPOCH] [--lr LR]\n",
            "                             [--log_interval LOG_INTERVAL]\n",
            "                             [--restore_file RESTORE_FILE]\n",
            "                             [--data_dir DATA_DIR] [--output_dir OUTPUT_DIR]\n",
            "                             [--results_file RESULTS_FILE] [--no_cuda]\n",
            "                             [--flip_toy_var_order] [--seed SEED]\n",
            "                             [--n_blocks N_BLOCKS]\n",
            "                             [--n_components N_COMPONENTS]\n",
            "                             [--hidden_size HIDDEN_SIZE] [--n_hidden N_HIDDEN]\n",
            "                             [--no_batch_norm]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-c4d00431-8db1-4e7e-9db4-7fed78a00a23.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as D\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import pprint\n",
        "import copy\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "# Train model parameter settings\n",
        "parser.add_argument('--train', default=True,action='store_true', help='if the model needs to be trained')\n",
        "parser.add_argument('--evaluate',default=True, action='store_true', help='if it need to be verified')\n",
        "parser.add_argument('--generate', default=True, action='store_true', help='generate samples')\n",
        "# model \n",
        "parser.add_argument('--model', default='realnvp', help='model name')\n",
        "# dataset\n",
        "parser.add_argument('--dataset', default='MNIST', help='dataset name')\n",
        "# Training parameter settings\n",
        "parser.add_argument('--batch_size', type=int, default=32)\n",
        "parser.add_argument('--n_epochs', type=int, default=50)\n",
        "parser.add_argument('--start_epoch', default=0, help='epoch settings')\n",
        "parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
        "parser.add_argument('--log_interval', type=int, default=1000, help='loss')\n",
        "\n",
        "# some default settings\n",
        "parser.add_argument('--restore_file',type=str, help='model save file')\n",
        "parser.add_argument('--data_dir', default='./data/', help='direction of dataset')\n",
        "parser.add_argument('--output_dir', default='./results/',help='direction of dataset')\n",
        "parser.add_argument('--results_file', default='results.txt', help='save results file name')\n",
        "parser.add_argument('--no_cuda', action='store_true', help='cuda')\n",
        "parser.add_argument('--flip_toy_var_order', action='store_true', help='whether to flip the toy dataset variable order to (x2,x1)')\n",
        "parser.add_argument('--seed', type=int, default=1, help='random seed parameter')\n",
        "\n",
        "# Model parameter settings\n",
        "parser.add_argument('--n_blocks', type=int, default=5, help='The number of blocks to stack in the model')\n",
        "parser.add_argument('--n_components', type=int, default=1, help='The number of Gaussian clusters for the Gaussian mixture model.')\n",
        "parser.add_argument('--hidden_size', type=int, default=100, help='hidden layer size')\n",
        "parser.add_argument('--n_hidden', type=int, default=1, help='number of hidden layer')\n",
        "parser.add_argument('--no_batch_norm', action='store_true')\n",
        "\n",
        "# RealNVP Coupling\n",
        "class LinearMaskedCoupling(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_hidden, mask, cond_label_size=None):\n",
        "        super().__init__()\n",
        "        self.register_buffer('mask', mask)\n",
        "        s_net = [nn.Linear(input_size + (cond_label_size if cond_label_size is not None else 0), hidden_size)]\n",
        "        for _ in range(n_hidden):\n",
        "            s_net += [nn.Tanh(), nn.Linear(hidden_size, hidden_size)]\n",
        "        s_net += [nn.Tanh(), nn.Linear(hidden_size, input_size)]\n",
        "        self.s_net = nn.Sequential(*s_net)\n",
        "        self.t_net = copy.deepcopy(self.s_net)\n",
        "        for i in range(len(self.t_net)):\n",
        "            if not isinstance(self.t_net[i], nn.Linear): self.t_net[i] = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        mx = x * self.mask\n",
        "        s = self.s_net(mx if y is None else torch.cat([y, mx], dim=1))\n",
        "        t = self.t_net(mx if y is None else torch.cat([y, mx], dim=1))\n",
        "        u = mx + (1 - self.mask) * (x - t) * torch.exp(-s)\n",
        "        log_abs_det_jacobian = - (1 - self.mask) * s\n",
        "        return u, log_abs_det_jacobian\n",
        "\n",
        "    def inverse(self, u, y=None):\n",
        "        mu = u * self.mask\n",
        "        s = self.s_net(mu if y is None else torch.cat([y, mu], dim=1))\n",
        "        t = self.t_net(mu if y is None else torch.cat([y, mu], dim=1))\n",
        "        x = mu + (1 - self.mask) * (u * s.exp() + t)  # cf RealNVP eq 7\n",
        "        log_abs_det_jacobian = (1 - self.mask) * s\n",
        "        return x, log_abs_det_jacobian\n",
        "\n",
        "# RealNVP BatchNorm\n",
        "class BatchNorm(nn.Module):\n",
        "    def __init__(self, input_size, momentum=0.9, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "        self.log_gamma = nn.Parameter(torch.zeros(input_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(input_size))\n",
        "        self.register_buffer('running_mean', torch.zeros(input_size))\n",
        "        self.register_buffer('running_var', torch.ones(input_size))\n",
        "\n",
        "    def forward(self, x, cond_y=None):\n",
        "        if self.training:\n",
        "            self.batch_mean = x.mean(0)\n",
        "            self.batch_var = x.var(0)\n",
        "            # Update average\n",
        "            self.running_mean.mul_(self.momentum).add_(self.batch_mean.data * (1 - self.momentum))\n",
        "            self.running_var.mul_(self.momentum).add_(self.batch_var.data * (1 - self.momentum))\n",
        "            mean = self.batch_mean\n",
        "            var = self.batch_var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "        # Compute normalized input\n",
        "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        y = self.log_gamma.exp() * x_hat + self.beta\n",
        "\n",
        "        log_abs_det_jacobian = self.log_gamma - 0.5 * torch.log(var + self.eps)\n",
        "        return y, log_abs_det_jacobian.expand_as(x)\n",
        "\n",
        "    def inverse(self, y, cond_y=None):\n",
        "        if self.training:\n",
        "            mean = self.batch_mean\n",
        "            var = self.batch_var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        x_hat = (y - self.beta) * torch.exp(-self.log_gamma)\n",
        "        x = x_hat * torch.sqrt(var + self.eps) + mean\n",
        "\n",
        "        log_abs_det_jacobian = 0.5 * torch.log(var + self.eps) - self.log_gamma\n",
        "        return x, log_abs_det_jacobian.expand_as(x)\n",
        "\n",
        "# Layers for Normalizing Streams\n",
        "class FlowSequential(nn.Sequential):\n",
        "    def forward(self, x, y):\n",
        "        sum_log_abs_det_jacobians = 0\n",
        "        for module in self:\n",
        "            x, log_abs_det_jacobian = module(x, y)\n",
        "            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
        "        return x, sum_log_abs_det_jacobians\n",
        "\n",
        "    def inverse(self, u, y):\n",
        "        sum_log_abs_det_jacobians = 0\n",
        "        for module in reversed(self):\n",
        "            u, log_abs_det_jacobian = module.inverse(u, y)\n",
        "            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
        "        return u, sum_log_abs_det_jacobians\n",
        "\n",
        "# Model settings\n",
        "class RealNVP(nn.Module):\n",
        "    def __init__(self, n_blocks, input_size, hidden_size, n_hidden, cond_label_size=None, batch_norm=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
        "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
        "\n",
        "        # construct model\n",
        "        modules = []\n",
        "        mask = torch.arange(input_size).float() % 2\n",
        "        for i in range(n_blocks):\n",
        "            modules += [LinearMaskedCoupling(input_size, hidden_size, n_hidden, mask, cond_label_size)]\n",
        "            mask = 1 - mask\n",
        "            modules += batch_norm * [BatchNorm(input_size)]\n",
        "\n",
        "        self.net = FlowSequential(*modules)\n",
        "\n",
        "    @property\n",
        "    def base_dist(self):\n",
        "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        return self.net(x, y)\n",
        "\n",
        "    def inverse(self, u, y=None):\n",
        "        return self.net.inverse(u, y)\n",
        "\n",
        "    def log_prob(self, x, y=None):\n",
        "        u, sum_log_abs_det_jacobians = self.forward(x, y)\n",
        "        return torch.sum(self.base_dist.log_prob(u) + sum_log_abs_det_jacobians, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "# traing and evaluate\n",
        "def train(model, dataloader, optimizer, epoch, args):\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "        model.train()\n",
        "\n",
        "        # check if labeled dataset\n",
        "        if len(data) == 1:\n",
        "            x, y = data[0], None\n",
        "        else:\n",
        "            x, y = data\n",
        "            y = y.to(args.device)\n",
        "        x = x.view(x.shape[0], -1).to(args.device)\n",
        "\n",
        "        loss = - model.log_prob(x, y if args.cond_label_size else None).mean(0)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % args.log_interval == 0:\n",
        "            print('epoch {:3d} / {}, step {:4d} / {}; loss {:.4f}'.format(\n",
        "                epoch, args.start_epoch + args.n_epochs, i, len(dataloader), loss.item()))\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, epoch, args):\n",
        "    model.eval()\n",
        "    # conditional model\n",
        "    if args.cond_label_size is not None:\n",
        "        logprior = torch.tensor(1 / args.cond_label_size).log().to(args.device)\n",
        "        loglike = [[] for _ in range(args.cond_label_size)]\n",
        "\n",
        "        for i in range(args.cond_label_size):\n",
        "            # transfer to onehot encoding\n",
        "            labels = torch.zeros(args.batch_size, args.cond_label_size).to(args.device)\n",
        "            labels[:,i] = 1\n",
        "\n",
        "            for x, y in dataloader:\n",
        "                x = x.view(x.shape[0], -1).to(args.device)\n",
        "                loglike[i].append(model.log_prob(x, labels))\n",
        "\n",
        "            loglike[i] = torch.cat(loglike[i], dim=0)\n",
        "        loglike = torch.stack(loglike, dim=1)\n",
        "        logprobs = logprior + loglike.logsumexp(dim=1)\n",
        "    # unconditional model\n",
        "    else:\n",
        "        logprobs = []\n",
        "        for data in dataloader:\n",
        "            x = data[0].view(data[0].shape[0], -1).to(args.device)\n",
        "            logprobs.append(model.log_prob(x))\n",
        "        logprobs = torch.cat(logprobs, dim=0).to(args.device)\n",
        "\n",
        "    logprob_mean, logprob_std = logprobs.mean(0), 2 * logprobs.var(0).sqrt() / math.sqrt(len(dataloader.dataset))\n",
        "    output = 'Evaluate ' + (epoch != None)*'(epoch {}) -- '.format(epoch) + 'logp(x) = {:.3f} +/- {:.3f}'.format(logprob_mean, logprob_std)\n",
        "    print(output)\n",
        "    print(output, file=open(args.results_file, 'a'))\n",
        "    return logprob_mean, logprob_std\n",
        "\n",
        "# generate plots\n",
        "@torch.no_grad()\n",
        "def generate(model, dataset_lam, args, step=None, n_row=10):\n",
        "    model.eval()\n",
        "    # conditional model\n",
        "    if args.cond_label_size:\n",
        "        samples = []\n",
        "        labels = torch.eye(args.cond_label_size).to(args.device)\n",
        "\n",
        "        for i in range(args.cond_label_size):\n",
        "            # The sample model base distribution and run through the inverse model to the sample set\n",
        "            u = model.base_dist.sample((n_row, args.n_components)).squeeze()\n",
        "            labels_i = labels[i].expand(n_row, -1)\n",
        "            sample, _ = model.inverse(u, labels_i)\n",
        "            # Sort by log_prob\n",
        "            log_probs = model.log_prob(sample, labels_i).sort(0)[1].flip(0)  \n",
        "            samples.append(sample[log_probs])\n",
        "\n",
        "        samples = torch.cat(samples, dim=0)\n",
        "\n",
        "    # unconditional model\n",
        "    else:\n",
        "        u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
        "        samples, _ = model.inverse(u)\n",
        "        log_probs = model.log_prob(samples).sort(0)[1].flip(0)\n",
        "        samples = samples[log_probs]\n",
        "\n",
        "    # save figures\n",
        "    samples = samples.view(samples.shape[0], *args.input_dims)\n",
        "    samples = (torch.sigmoid(samples) - dataset_lam) / (1 - 2 * dataset_lam)\n",
        "    filename = 'generate_samples_with_' + (step != None)*'_epoch_{}'.format(step) + '.png'\n",
        "    save_image(samples, os.path.join(args.output_dir, filename), nrow=n_row, normalize=True)\n",
        "\n",
        "def train_and_evaluate(model, train_loader, test_loader, optimizer, args):\n",
        "    best_eval_logprob = float('-inf')\n",
        "    for i in range(args.start_epoch, args.start_epoch + args.n_epochs):\n",
        "        train(model, train_loader, optimizer, i, args)\n",
        "        eval_logprob, _ = evaluate(model, test_loader, i, args)\n",
        "\n",
        "        # Save training checkpoint\n",
        "        torch.save({'epoch': i,\n",
        "                    'model_state': model.state_dict(),\n",
        "                    'optimizer_state': optimizer.state_dict()},\n",
        "                    os.path.join(args.output_dir, 'model_checkpoint.pt'))\n",
        "        # save model\n",
        "        torch.save(model.state_dict(), os.path.join(args.output_dir, 'model_state.pt'))\n",
        "\n",
        "        # save optimal model\n",
        "        if eval_logprob > best_eval_logprob:\n",
        "            best_eval_logprob = eval_logprob\n",
        "            torch.save({'epoch': i,\n",
        "                        'model_state': model.state_dict(),\n",
        "                        'optimizer_state': optimizer.state_dict()},\n",
        "                        os.path.join(args.output_dir, 'best_model_checkpoint.pt'))\n",
        "\n",
        "        # generate figures\n",
        "        if args.dataset == 'MNIST':\n",
        "            generate(model, train_loader.dataset.lam, args, step=i)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    if not os.path.isdir(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    # CPU/GPU\n",
        "    args.device = torch.device('cuda:0' if torch.cuda.is_available() and not args.no_cuda else 'cpu')\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.device.type == 'cuda': torch.cuda.manual_seed(args.seed)\n",
        "    # load dataset\n",
        "    train_dataloader, test_dataloader = fetch_dataloaders(args.dataset, args.batch_size, args.device, args.flip_toy_var_order)\n",
        "    args.input_size = train_dataloader.dataset.input_size\n",
        "    args.input_dims = train_dataloader.dataset.input_dims\n",
        "    args.cond_label_size = None\n",
        "\n",
        "    # model settings\n",
        "    if args.model =='realnvp':\n",
        "        model = RealNVP(args.n_blocks, args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
        "                        batch_norm=not args.no_batch_norm)\n",
        "    else:\n",
        "        raise ValueError('Please use a model of normalizing flows')\n",
        "\n",
        "    model = model.to(args.device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)\n",
        "\n",
        "    if args.restore_file:\n",
        "        # Model and optimizer status\n",
        "        state = torch.load(args.restore_file, map_location=args.device)\n",
        "        model.load_state_dict(state['model_state'])\n",
        "        optimizer.load_state_dict(state['optimizer_state'])\n",
        "        args.start_epoch = state['epoch'] + 1\n",
        "        # set up paths\n",
        "        args.output_dir = os.path.dirname(args.restore_file)\n",
        "    args.results_file = os.path.join(args.output_dir, args.results_file)\n",
        "\n",
        "    print('加载参数设置:')\n",
        "    print(pprint.pformat(args.__dict__))\n",
        "    print(model)\n",
        "    print(pprint.pformat(args.__dict__), file=open(args.results_file, 'a'))\n",
        "    print(model, file=open(args.results_file, 'a'))\n",
        "    # train model\n",
        "    if args.train:\n",
        "        train_and_evaluate(model, train_dataloader, test_dataloader, optimizer, args)\n",
        "\n",
        "    # evalutae model\n",
        "    if args.evaluate:\n",
        "        evaluate(model, test_dataloader, None, args)\n",
        "\n",
        "    # generate figures\n",
        "    if args.generate:\n",
        "        if args.dataset == 'MNIST':\n",
        "            generate(model, train_dataloader.dataset.lam, args)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "realnvp_cl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}